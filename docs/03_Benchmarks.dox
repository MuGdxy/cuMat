namespace cuMat {

/** \page Benchmarks Benchmarks

To elaborate the execution time of the cuMat operations, we compare the implementations of the same algorithm in cuMat, cuBLAS, Eigen, numpy and tensorflow.

\section System System setup

The operation system used to execute the benchmarks:
 - Windows 10 Home
 - CPU: Intel(R) Core(TM) i7-6700, 3.40GHz x 8
 - 16.00GB RAM
 - GPU: NVidia GeForce GTX 1060 6GB
 - Visual Studio Enterprise 2017
 - CUDA SDK 9.2
 - Python: Python 3.5.2, 64-bit
 - Numpy version 1.12.0

\section LinearCombination Benchmark 1: linear combination

The first benchmark measures the performance of a linear combination (series of AXPY's)
\f[
    v = \sum_{i=1}^k \alpha_i v_i
\f]
with \f$\alpha_i \in \mathbb{R}\f$ and \f$v, v_i \in \mathbb{R}^n\f$. 
The source code for the benchmarks can be found in the folder \c benchmarks/linear-combination.

First test case: constant number of combinations (\f$k=2\f$), varying size of the vectors (\f$n\f$).

\htmlonly <style>div.image img[src="Linear Combination - Constant Count.png"]{width:500px;}</style> \endhtmlonly
\image html "Linear Combination - Constant Count.png"
\image latex "Linear Combination - Constant Count.png" width=10cm

You can see that the pure CPU libraries (numpy+Eigen) are faster than the pure GPU libraries (cuMat+cuBLAS) for small vector sizes, smaller than 10000 entries.
After that sweep spot, the GPU is better saturated and the performance of cuMat and cuBLAS is better than of Eigen or cuMat. For the largest case of 50000000 entries, cuMat is about 23 times faster than numpy and more than 100 times faster than Eigen.
In this basic case, however, cuBLAS outperforms our custom AXPY-implementation.

Second test case: constant size of the vectors (\f$n=1000000\f$), varying number of combinations (\f$k\f$)

\htmlonly <style>div.image img[src="Linear Combination - Constant Size.png"]{width:500px;}</style> \endhtmlonly
\image html "Linear Combination - Constant Size.png"
\image latex "Linear Combination - Constant Size.png" width=10cm

This test case shows the power of the kernel merging performed by cuMat. The linear combination is evaluated in a single kernel in cuMat (without storing the intermediate results in memory), while cuBLAS needs one call to AXPY per factor (and writes the intermediate results into memory every time).
Between 3 and 4 linear combinations, cuMat becomes faster than cuBLAS. 

\section Benchmark_CSRMV Benchmark 2: Sparse Matrix - Vector multiplication

Next we compute the performance of our custom sparse matrix (CSR-format) - vector multiplication routine with the implementations in Eigen and in cuSparse.
The matrix is a 2D poisson matrix with increasing grid size.
Our implementation achieves even a slightly better performance than the optimized routine provided by NVIDIAâ€™s cuSparse library and is 10x faster than Eigen.

\htmlonly <style>div.image img[src="CSRMV - 2D-Poisson Matrix.png"]{width:500px;}</style> \endhtmlonly
\image html "CSRMV - 2D-Poisson Matrix.png"
\image latex "CSRMV - 2D-Poisson Matrix.png" width=10cm

\section Benchmark_CG Benchmark 3: Conjugate Gradient Solver

In a last benchmark we compare our implementation of the Conjugate Gradient Solver with the implementation shipped with Eigen.
As a model problem, a 2D diffusion process with random Dirichlet and Neumann boundaries is solved.
For large enough problems, our GPU implementation is more than 10x as fast as the Eigen implementation. This is because the sparse matrix-vector multiplication is faster on the GPU than on the CPU as shown before, and for large problems the memory transfer from device to host to query the current error is not the bottleneck.
\htmlonly <style>div.image img[src="Conjugate Gradient - 2D-Poisson Matrix.png"]{width:500px;}</style> \endhtmlonly
\image html "Conjugate Gradient - 2D-Poisson Matrix.png"
\image latex "Conjugate Gradient - 2D-Poisson Matrix" width=10cm

*/

}
