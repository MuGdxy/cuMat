namespace cuMat {

/** \page Benchmarks Benchmarks

To elaborate the execution time of the cuMat operations, we compare the implementations of the same algorithm in cuMat, cuBLAS, Eigen, numpy and tensorflow.

The source code can be found under the "benchmarks" folder in the cuMat root directory. This folder also contains the tables with the raw timings in json format.

\section System System setup

The operation system used to execute the benchmarks:
 - Windows 10 Home
 - Intel Xeon W-2123 @ 3.60GHz
 - 64.00GB RAM
 - NVIDIA GeForce RTX 2070
 - Visual Studio Enterprise 2017
 - CUDA SDK 10.0
 - Python: Python 3.6.6, 64-bit
 - Numpy version 1.12.0
 - Eigen 3.3.2 with MKL enabled

\section LinearCombination Benchmark 1: linear combination

The first benchmark measures the performance of a linear combination (series of AXPY's)
\f[
    v = \sum_{i=1}^k \alpha_i v_i
\f]
with \f$\alpha_i \in \mathbb{R}\f$ and \f$v, v_i \in \mathbb{R}^n\f$. 
The source code for the benchmarks can be found in the folder \c benchmarks/linear-combination.

First test case: constant number of combinations (\f$k=2\f$), varying size of the vectors (\f$n\f$).

\htmlonly <style>div.image img[src="Linear Combination - Constant Count.png"]{width:500px;}</style> \endhtmlonly
\image html "Linear Combination - Constant Count.png"
\image latex "Linear Combination - Constant Count.png" width=10cm

You can see that the pure CPU libraries (numpy+Eigen) are faster than the pure GPU libraries (cuMat+cuBLAS) for small vector sizes, smaller than 10000 entries.
After that sweep spot, the GPU is better saturated and the performance of cuMat and cuBLAS is better than of Eigen or cuMat. For the largest case of 50000000 entries, cuMat is about 23 times faster than numpy and more than 100 times faster than Eigen.
In this basic case, however, cuBLAS outperforms our custom AXPY-implementation.

Second test case: constant size of the vectors (\f$n=1000000\f$), varying number of combinations (\f$k\f$)

\htmlonly <style>div.image img[src="Linear Combination - Constant Size.png"]{width:500px;}</style> \endhtmlonly
\image html "Linear Combination - Constant Size.png"
\image latex "Linear Combination - Constant Size.png" width=10cm

This test case shows the power of the kernel merging performed by cuMat. The linear combination is evaluated in a single kernel in cuMat (without storing the intermediate results in memory), while cuBLAS needs one call to AXPY per factor (and writes the intermediate results into memory every time).
Between 3 and 4 linear combinations, cuMat becomes faster than cuBLAS. 

\section Benchmark_Dotproduct Benchmark 2: Dotproduct

As a demonstration of our reduction API we evaluate the performance of the inner product (scalar/dot product) of two vectors.
We compare our implementation to Eigen, cuBLAS, Thrust and CUB.

\htmlonly <style>div.image img[src="Dotproduct.png"]{width:500px;}</style> \endhtmlonly
\image html "Dotproduct.png"
\image latex "Dotproduct.png" width=10cm

One can see that all GPU versions are almost exactly equal. One might say that our implementation of the reduction is a bit slower than cuBLAS or CUB, but this is due to a little overhead of index computations. 
These index computations, however, allow the reduction kernels to perform reductions over arbitrary axises and arbitrary complex operations.

\section Benchmark_CSRMV Benchmark 3: Sparse Matrix - Vector multiplication

Next we compute the performance of our custom sparse matrix (CSR-format) - vector multiplication routine with the implementations in Eigen and in cuSparse.
The matrix is a 2D poisson matrix with increasing grid size.
Our implementation achieves even a slightly better performance than the optimized routine provided by NVIDIA's cuSparse library and is 10x faster than Eigen.

\htmlonly <style>div.image img[src="CSRMV - 2D-Poisson Matrix.png"]{width:500px;}</style> \endhtmlonly
\image html "CSRMV - 2D-Poisson Matrix.png"
\image latex "CSRMV - 2D-Poisson Matrix.png" width=10cm

\section Benchmark_CG Benchmark 4: Conjugate Gradient Solver

In this benchmark we compare our implementation of the Conjugate Gradient Solver with the implementation shipped with Eigen.
As a model problem, a 2D diffusion process with random Dirichlet and Neumann boundaries is solved.
For large enough problems, our GPU implementation is more than 20x as fast as the Eigen implementation.
This is because the sparse matrix-vector multiplication and reductions are much faster on the GPU than on the CPU as shown before, and for large problems the memory transfer from device to host to query the current error is not the bottleneck.
\htmlonly <style>div.image img[src="Conjugate Gradient - 2D-Poisson Matrix.png"]{width:500px;}</style> \endhtmlonly
\image html "Conjugate Gradient - 2D-Poisson Matrix.png"
\image latex "Conjugate Gradient - 2D-Poisson Matrix" width=10cm

\section Benchmark_GMM Benchmark 5: Gaussian Mixture Model

This last benchmarks can also be seen as a demonstration of how a larger application can be build with cuMat.

The starting point was an implementation of a Gaussian Mixture Model with the EM algorithm. This implementation was then ported to cuMat by changing all instances <code>Eigen::Matrix</code> to <code>cuMat::Matrix</code>.
Followed by converting all explicit loops in the Eigen version into single expressions with the powerful broadcasting and batch evaluation semantic of cuMat.

As a validation, here is a 2D test case with 4 components, 50 components and 20 EM iterations. One can see that the Eigen and cuMat implementation produce exactly the same results, given the same starting values.
\htmlonly <style>div.image img[src="GMM_Test.png"]{width:500px;}</style> \endhtmlonly
\image html "GMM_Test.png"
\image latex "GMM_Test" width=10cm

For the benchmarks, the performance is evaluated if one of the parameters (number of dimensions, number of components, number of points) is varied. One can see that the GPU implementation with cuMat always outperforms the CPU implementation with Eigen, except for very small number of points.
\htmlonly <style>div.image img[src="GMM_Scale-Dimensions.png"]{width:500px;}</style> \endhtmlonly
\image html "GMM_Scale-Dimensions.png"
\image latex "GMM_Scale-Dimensions" width=10cm
\htmlonly <style>div.image img[src="GMM_Scale-Num-Components.png"]{width:500px;}</style> \endhtmlonly
\image html "GMM_Scale-Num-Components.png"
\image latex "GMM_Scale-Num-Components" width=10cm
\htmlonly <style>div.image img[src="GMM_Scale-Points.png"]{width:500px;}</style> \endhtmlonly
\image html "GMM_Scale-Points.png"
\image latex "GMM_Scale-Points" width=10cm
*/

}
